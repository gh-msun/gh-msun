{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "import regex as re\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import IntegerType, LongType, ArrayType, StringType, DoubleType\n",
    "from pyspark.sql.functions import udf, explode, broadcast, count, lit, length, col, monotonically_increasing_id\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# UPDATE HOME!\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark\"\n",
    "# THIS needs to be set-up before running the notebook\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"/temp\"\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.ui.showConsoleProgress\", \"True\")\n",
    "spark_conf.set(\"spark.executor.instances\", \"2\")\n",
    "spark_conf.set(\"spark.executor.cores\", \"2\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"64g\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "spark_conf.set(\"spark.parquet.filterPushdown\", \"true\")\n",
    "spark_conf.set(\"spark.local.dir\", \"/temp\")\n",
    "spark_conf.getAll()\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling in PySpark\n",
    "The `sample()` function takes fraction of reads to sample, not the number of reads to sample. \\\n",
    "We can compute the fraction from the total number of reads and the number of desired reads to sample. \\\n",
    "Mapping: `(N rows to sample) --> (F fraction to sample)`\n",
    "```\n",
    "N rows to sample = fraction * total reads\n",
    "fraction = N rows to sample / total reads\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_celltypes_helper(cell_types, total_reads_to_sample, proportions, seed, parquet_path, result_path, save=False, itr=None):\n",
    "    ''' Mix reads from different cell types based on given proportion and total reads to sample.\n",
    "    \n",
    "    Arguments:\n",
    "    cell_types -- list of cell type to mix\n",
    "    total_reads_to_sample -- integer representing the total number of reads to sample across all cell types\n",
    "    proportions -- list of proportions to sample for each cell type\n",
    "    seed -- seed for .sample()\n",
    "    parquet_path -- string of path to the directory with source cell type reads to mix from\n",
    "    result_path -- string of path to output parquet file\n",
    "    itr -- mixture iteration for creating multiple mixtures\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    combined_df -- pyspark.sql.dataframe.DataFrame\n",
    "    '''\n",
    "    \n",
    "    print(f'--> seed: {seed}')\n",
    "    \n",
    "    # load the parquet files for selected cell types & count rows\n",
    "    parquet_df = []\n",
    "    total_reads_per_celltype = []\n",
    "    \n",
    "    print('--> Load parquet files and count rows...')\n",
    "    for cell_type in cell_types:\n",
    "        print(f'----------> Loading cell type: ### {cell_type}')\n",
    "        df = spark.read.parquet(f'{parquet_path}collapsed_reads_{cell_type}/')\n",
    "        parquet_df.append(df)\n",
    "        total_reads_per_celltype.append(df.count())\n",
    "\n",
    "    total_reads_per_celltype = np.array(total_reads_per_celltype)\n",
    "    \n",
    "    # compute fraction to sample for each cell type (later convert to index)\n",
    "    n_reads_to_sample = proportions * total_reads_to_sample\n",
    "    sampling_fraction = n_reads_to_sample / total_reads_per_celltype\n",
    "    print(f'total_reads_to_sample: {total_reads_per_celltype}')\n",
    "    print(f'Sampling fraction: {sampling_fraction}')\n",
    "    \n",
    "    # sample reads from each cell type\n",
    "    sampled_df = []\n",
    "    \n",
    "    print('--> Sample rows for each cell type...')\n",
    "    for i in range(0, len(cell_types)):\n",
    "        print(f'----------> Sampling cell type: {i}')\n",
    "        df = parquet_df[i]\n",
    "        frac = sampling_fraction[i]\n",
    "        df_sample = df.sample(False, frac, seed)\n",
    "        sampled_df.append(df_sample)\n",
    "        n_sampled = df_sample.count()\n",
    "        print(f'----------> {n_sampled}')\n",
    "    \n",
    "    # combine reads\n",
    "    print('--> Combining sampled reads into one dataframe...')\n",
    "    combined_df = functools.reduce(DataFrame.union, sampled_df)\n",
    "    \n",
    "    if save:\n",
    "        # create file name \n",
    "        seed_string = str(int(seed))\n",
    "        celltype_string = '_'.join(cell_types)\n",
    "        proportion_str = [str(i) for i in proportions]\n",
    "        proportion_string = '_'.join(proportion_str)\n",
    "        mixture_itr = f'M{itr}_'\n",
    "        file_name = mixture_itr + \\\n",
    "                    celltype_string + \\\n",
    "                    '_proportions_' + proportion_string + \\\n",
    "                    f'_seed_{seed_string}' + \\\n",
    "                    '.parquet/'\n",
    "\n",
    "        print('--> Saving parquet file...')\n",
    "        save_path = result_path + file_name\n",
    "        combined_df.write.mode('overwrite').parquet(save_path)\n",
    "        print(f'--> Saved to: {save_path}')\n",
    "    \n",
    "    return(combined_df)\n",
    "\n",
    "\n",
    "def mix_celltypes(n, cell_types, total_reads_to_sample, proportions, seed, parquet_path, result_path, save=False):\n",
    "    ''' Mix reads from different cell types based on given proportion and total reads to sample.\n",
    "    \n",
    "    Arguments:\n",
    "    n -- total number of mixtures to make\n",
    "    cell_types -- list of cell type to mix\n",
    "    total_reads_to_sample -- integer representing the total number of reads to sample across all cell types\n",
    "    proportions -- list of proportions to sample for each cell type\n",
    "    seed -- seed for .sample()\n",
    "    parquet_path -- string of path to the directory with source cell type reads to mix from\n",
    "    result_path -- string of path to output parquet file\n",
    "    \n",
    "    Output:\n",
    "    combined_df -- pyspark.sql.dataframe.DataFrame\n",
    "    '''\n",
    "\n",
    "    # Generate seeds (assume we want values between 0 and 1 million)\n",
    "    random.seed(seed)\n",
    "    seeds = [random.randint(0, 10**6) for _ in range(n)]\n",
    "    \n",
    "    # Create n mixtures\n",
    "    mixtures = []\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        \n",
    "        print(f'################ Creating mixture {i}... ################')\n",
    "        mixture = mix_celltypes_helper(cell_types=cell_types,\n",
    "                                       total_reads_to_sample=total_reads_to_sample, \n",
    "                                       proportions=proportions, \n",
    "                                       seed=seeds[i],\n",
    "                                       parquet_path=parquet_path,\n",
    "                                       result_path=result_path,\n",
    "                                       save=save,\n",
    "                                       itr=i)\n",
    "        mixtures.append(mixture)\n",
    "        print(' ')\n",
    "    \n",
    "    print(f'>>> Complete. <<<')\n",
    "    return(mixtures) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mix_celltypes_index(cell_types, total_reads_to_sample, proportions, seed, parquet_path, result_path, save=False):\n",
    "#     ''' Mix reads from different cell types based on given proportion and total reads to sample.\n",
    "    \n",
    "#     Arguments:\n",
    "#     cell_types -- list of cell type to mix\n",
    "#     total_reads_to_sample -- integer representing the total number of reads to sample across all cell types\n",
    "#     proportions -- list of proportions to sample for each cell type\n",
    "#     seed -- seed for .sample()\n",
    "#     parquet_path -- string of path to the directory with source cell type reads to mix from\n",
    "#     result_path -- string of path to output parquet file\n",
    "    \n",
    "#     Output:\n",
    "#     combined_df -- pyspark.sql.dataframe.DataFrame\n",
    "#     '''\n",
    "    \n",
    "#     # load the parquet files for selected cell types & count rows\n",
    "#     sampled_df = []\n",
    "#     i = 0\n",
    "    \n",
    "#     for cell_type in cell_types:\n",
    "#         print(f'--> Loading cell type: ### {cell_type}')\n",
    "#         df = spark.read.parquet(f'{parquet_path}collapsed_reads_{cell_type}/')\n",
    "        \n",
    "#         print(f'--> Sampling cell type: {cell_type}')\n",
    "#         df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "#         df_nrow = df.count()\n",
    "#         n_sample = round(df_nrow * proportions[i])\n",
    "#         indices_to_sample_from = list(range(0, df_nrow))\n",
    "#         indices_sampled = random.sample(indices_to_sample_from, n_sample)\n",
    "#         df_sampled = df[df.index.isin(indices_sampled)]\n",
    "        \n",
    "#         sampled_df.append(df_sampled)\n",
    "#         i+=1\n",
    "\n",
    "#     # combine reads\n",
    "#     print('--> Combining sampled reads into one dataframe...')\n",
    "#     combined_df = functools.reduce(DataFrame.union, sampled_df)\n",
    "    \n",
    "#     if save:\n",
    "#         # create file name \n",
    "#         seed_string = str(int(seed))\n",
    "#         celltype_string = '_'.join(cell_types)\n",
    "#         proportion_str = [str(i) for i in proportions]\n",
    "#         proportion_string = '_'.join(proportion_str)\n",
    "#         file_name =  celltype_string + \\\n",
    "#                     '_proportions_' + proportion_string + \\\n",
    "#                     f'_seed_{seed_string}' + \\\n",
    "#                     '.parquet/'\n",
    "\n",
    "#         print('--> Saving parquet file...')\n",
    "#         save_path = result_path + file_name\n",
    "#         combined_df.write.mode('overwrite').parquet(save_path)\n",
    "#         print(f'--> Saved to: {save_path}')\n",
    "    \n",
    "#     return(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Creating mixture 0... ################\n",
      "--> seed: 83723\n",
      "--> Load parquet files and count rows...\n",
      "----------> Loading cell type: ### Blueprint-B\n",
      "----------> Loading cell type: ### Blueprint-CD4\n",
      "total_reads_to_sample: [2963964 1965191]\n",
      "Sampling fraction: [0.00168693 0.00254428]\n",
      "--> Sample rows for each cell type...\n",
      "----------> Sampling cell type: 0\n",
      "----------> 4998\n",
      "----------> Sampling cell type: 1\n",
      "----------> 5030\n",
      "--> Combining sampled reads into one dataframe...\n",
      "--> Saving parquet file...\n",
      "--> Saved to: /analysis/gh-msun/projects/2023_06_26_SRT_deconvolution_MS/output/mixture/M0_Blueprint-B_Blueprint-CD4_proportions_0.5_0.5_seed_83723.parquet/\n",
      " \n",
      "################ Creating mixture 1... ################\n",
      "--> seed: 452891\n",
      "--> Load parquet files and count rows...\n",
      "----------> Loading cell type: ### Blueprint-B\n",
      "----------> Loading cell type: ### Blueprint-CD4\n",
      "total_reads_to_sample: [2963964 1965191]\n",
      "Sampling fraction: [0.00168693 0.00254428]\n",
      "--> Sample rows for each cell type...\n",
      "----------> Sampling cell type: 0\n",
      "----------> 4918\n",
      "----------> Sampling cell type: 1\n",
      "----------> 4905\n",
      "--> Combining sampled reads into one dataframe...\n",
      "--> Saving parquet file...\n",
      "--> Saved to: /analysis/gh-msun/projects/2023_06_26_SRT_deconvolution_MS/output/mixture/M1_Blueprint-B_Blueprint-CD4_proportions_0.5_0.5_seed_452891.parquet/\n",
      " \n",
      ">>> Complete. <<<\n"
     ]
    }
   ],
   "source": [
    "N=2\n",
    "CELLTYPES = ['Blueprint-B', 'Blueprint-CD4']\n",
    "TOTAL_READS_TO_SAMPLE = 10000\n",
    "PROPORTION = np.array([0.5, 0.5])\n",
    "SEED = 888\n",
    "PARQUET_PATH = '/analysis/gh-msun/projects/2023_06_26_SRT_deconvolution_MS/output/mixture_source/'\n",
    "RESULT_PATH = '/analysis/gh-msun/projects/2023_06_26_SRT_deconvolution_MS/output/mixture/'\n",
    "\n",
    "test_mixtures = mix_celltypes(n=N,\n",
    "                             cell_types=CELLTYPES,\n",
    "                             total_reads_to_sample=TOTAL_READS_TO_SAMPLE, \n",
    "                             proportions=PROPORTION, \n",
    "                             seed=SEED,\n",
    "                             parquet_path=PARQUET_PATH,\n",
    "                             result_path=RESULT_PATH,\n",
    "                             save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[sample_id: string, molecule_id: string, chr: string, number_molecules: bigint, cpg_index_min: bigint, cpg_index_max: bigint, pat_string: string, region_id: string, region_cpg_index_min: bigint, region_cpg_index_max: bigint],\n",
       " DataFrame[sample_id: string, molecule_id: string, chr: string, number_molecules: bigint, cpg_index_min: bigint, cpg_index_max: bigint, pat_string: string, region_id: string, region_cpg_index_min: bigint, region_cpg_index_max: bigint]]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999.99615748"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1965191 * 0.00254428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999.99979052"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2963964 * 0.00168693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----+----------------+-------------+-------------+----------------+--------------------+--------------------+--------------------+\n",
      "|sample_id|molecule_id|  chr|number_molecules|cpg_index_min|cpg_index_max|      pat_string|           region_id|region_cpg_index_min|region_cpg_index_max|\n",
      "+---------+-----------+-----+----------------+-------------+-------------+----------------+--------------------+--------------------+--------------------+\n",
      "|ERS666930|  410430713|chr16|               1|     21772142|     21772148|         CCC.CCC|Immune_Lymph_B_Na...|            21772140|            21772145|\n",
      "|ERS523625|  246240980|chr16|               1|     21773396|     21773407|    CCCCCCCCCCCC|Loyfer2022_Prepri...|            21773392|            21773411|\n",
      "|ERS214675|  225068591|chr15|               1|     21491132|     21491142|     CCCCCCCCCCC|Umbilical_Endothe...|            21491086|            21491152|\n",
      "|ERS214675|  225068617|chr15|               1|     21491138|     21491150|   TCCCCCCTCCCCC|Umbilical_Endothe...|            21491086|            21491152|\n",
      "|ERS523625|  243210870|chr15|               1|     21491106|     21491121|CCCCCTCCCCCCCCCC|Umbilical_Endothe...|            21491086|            21491152|\n",
      "+---------+-----------+-----+----------------+-------------+-------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mixtures[0].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----+----------------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sample_id|molecule_id|  chr|number_molecules|cpg_index_min|cpg_index_max|          pat_string|           region_id|region_cpg_index_min|region_cpg_index_max|\n",
      "+---------+-----------+-----+----------------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|ERS222266|  218899314|chr16|               1|     21773397|     21773413|   TCCCTCCCCCCTCCCCC|Loyfer2022_Prepri...|            21773392|            21773411|\n",
      "|ERS222206|  319856146|chr16|               1|     21773397|     21773418|CCCCCCCT......CTC...|Loyfer2022_Prepri...|            21773392|            21773411|\n",
      "|ERS666927|  429426016|chr16|               1|     21773402|     21773416|     CCCCCCTCCCCCTCC|Loyfer2022_Prepri...|            21773392|            21773411|\n",
      "|ERS337607|  395958219|chr16|               1|     21774674|     21774679|              CCCCCC|Immune_Refined_B_...|            21774666|            21774674|\n",
      "|ERS666931|  396008414|chr15|               1|     21491070|     21491092|TCTCCCCCTCCCCCCCC...|Umbilical_Endothe...|            21491086|            21491152|\n",
      "+---------+-----------+-----+----------------+-------------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mixtures[1].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
