{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "import regex as re\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import IntegerType, LongType, ArrayType, StringType, DoubleType\n",
    "from pyspark.sql.functions import udf, explode, broadcast, count, lit, length, col\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# UPDATE HOME!\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark\"\n",
    "# THIS needs to be set-up before running the notebook\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"/temp\"\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.ui.showConsoleProgress\", \"True\")\n",
    "spark_conf.set(\"spark.executor.instances\", \"2\")\n",
    "spark_conf.set(\"spark.executor.cores\", \"2\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"64g\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "spark_conf.set(\"spark.parquet.filterPushdown\", \"true\")\n",
    "spark_conf.set(\"spark.local.dir\", \"/temp\")\n",
    "spark_conf.getAll()\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = 'deconvolution_v2.v23_conv.with_cpg_index'\n",
    "REGION_BED_COLS = [\n",
    "    'region_chr', 'region_start', 'region_end', \n",
    "    'region_cpg_index_min', 'region_cpg_index_max', 'region_id'\n",
    "]\n",
    "FILTER_CG_COUNT = 3\n",
    "FILTER_CG_COUNT_REGION = 1\n",
    "\n",
    "#--- Local paths\n",
    "ROOT_DIR = '/analysis/gh-msun/projects'\n",
    "PROJECT_SLUG = '2023_06_26_SRT_deconvolution_MS'\n",
    "PROJECT_DIR = ROOT_DIR + '/{}'.format(PROJECT_SLUG)\n",
    "\n",
    "# Regions\n",
    "REGION_PATH = (\n",
    "    PROJECT_DIR + '/stage/panel_data/{regions}.bed'\n",
    ").format(regions=REGIONS)\n",
    "\n",
    "# CpG map; genomic coordinate to CpG index;\n",
    "CPG_MAP_PATH = PROJECT_DIR + '/stage/cpg_loci/cpg_loci_hg19.combined_annot.tsv.gz'\n",
    "\n",
    "#--- Where to store results\n",
    "RESULT_PATH = PROJECT_DIR + '/output/methyl_score/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CpG Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpg_map = pd.read_csv(CPG_MAP_PATH, usecols=['chr', 'start', 'end', 'cpg_index', 'cpg_index_hg38'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 7.43 s, total: 22.8 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridxs = ~cpg_map['cpg_index_hg38'].isna()\n",
    "hg19_hg38_map = dict(itertools.zip_longest(cpg_map[ridxs]['cpg_index'], cpg_map[ridxs]['cpg_index_hg38'].astype(int)))\n",
    "hg38_hg19_map = dict(itertools.zip_longest(cpg_map[ridxs]['cpg_index_hg38'].astype(int), cpg_map[ridxs]['cpg_index']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 1658)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df = pd.read_csv(REGION_PATH, sep='\\t', usecols=range(0, 6), names=REGION_BED_COLS)\n",
    "\n",
    "region_df['region_cpg_index_max'] -= 1\n",
    "region_df.sort_values('region_cpg_index_min', inplace=True)\n",
    "region_df['region_cpg_index_min_hg38'] = region_df['region_cpg_index_min'].map(hg19_hg38_map)\n",
    "region_df['region_cpg_index_max_hg38'] = region_df['region_cpg_index_max'].map(hg19_hg38_map)\n",
    "\n",
    "region_df.shape[0], region_df['region_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 1658)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridxs = ~region_df['region_cpg_index_min_hg38'].isna()\n",
    "ridxs &= ~region_df['region_cpg_index_max_hg38'].isna()\n",
    "region_df = region_df[ridxs].copy()\n",
    "region_df.shape[0], region_df['region_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1657, 1657)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg_count_hg19 = region_df['region_cpg_index_max']-region_df['region_cpg_index_min'] + 1\n",
    "cg_count_hg38 = region_df['region_cpg_index_max_hg38']-region_df['region_cpg_index_min_hg38'] + 1\n",
    "ridxs = (cg_count_hg19==cg_count_hg38)\n",
    "ridxs &= (cg_count_hg19>=FILTER_CG_COUNT_REGION)\n",
    "region_df = region_df[ridxs].copy()\n",
    "region_df.shape[0], region_df['region_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df['region_cpg_index_min_hg38'] = region_df['region_cpg_index_min_hg38'].astype(int)\n",
    "region_df['region_cpg_index_max_hg38'] = region_df['region_cpg_index_max_hg38'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_chr</th>\n",
       "      <th>region_start</th>\n",
       "      <th>region_end</th>\n",
       "      <th>region_cpg_index_min</th>\n",
       "      <th>region_cpg_index_max</th>\n",
       "      <th>region_id</th>\n",
       "      <th>region_cpg_index_min_hg38</th>\n",
       "      <th>region_cpg_index_max_hg38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1114771</td>\n",
       "      <td>1114971</td>\n",
       "      <td>20117</td>\n",
       "      <td>20129</td>\n",
       "      <td>Immune_Broad_B-chr1:1114772-1114971</td>\n",
       "      <td>21119</td>\n",
       "      <td>21131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1157450</td>\n",
       "      <td>1157720</td>\n",
       "      <td>21684</td>\n",
       "      <td>21703</td>\n",
       "      <td>Immune_Broad_NK-chr1:1157451-1157720</td>\n",
       "      <td>22686</td>\n",
       "      <td>22705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1157879</td>\n",
       "      <td>1158277</td>\n",
       "      <td>21710</td>\n",
       "      <td>21726</td>\n",
       "      <td>Immune_Broad_NK-chr1:1157880-1158277</td>\n",
       "      <td>22712</td>\n",
       "      <td>22728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chr1</td>\n",
       "      <td>6341182</td>\n",
       "      <td>6341377</td>\n",
       "      <td>140667</td>\n",
       "      <td>140681</td>\n",
       "      <td>Immune_Broad_Eosi-chr1:6341183-6341377</td>\n",
       "      <td>142368</td>\n",
       "      <td>142382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chr1</td>\n",
       "      <td>9147788</td>\n",
       "      <td>9147871</td>\n",
       "      <td>188605</td>\n",
       "      <td>188608</td>\n",
       "      <td>Immune_Broad_Neutro-chr1:9147789-9147871</td>\n",
       "      <td>190307</td>\n",
       "      <td>190310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region_chr  region_start  region_end  region_cpg_index_min  region_cpg_index_max                                 region_id  region_cpg_index_min_hg38  region_cpg_index_max_hg38\n",
       "0        chr1       1114771     1114971                 20117                 20129       Immune_Broad_B-chr1:1114772-1114971                      21119                      21131\n",
       "1        chr1       1157450     1157720                 21684                 21703      Immune_Broad_NK-chr1:1157451-1157720                      22686                      22705\n",
       "2        chr1       1157879     1158277                 21710                 21726      Immune_Broad_NK-chr1:1157880-1158277                      22712                      22728\n",
       "14       chr1       6341182     6341377                140667                140681    Immune_Broad_Eosi-chr1:6341183-6341377                     142368                     142382\n",
       "19       chr1       9147788     9147871                188605                188608  Immune_Broad_Neutro-chr1:9147789-9147871                     190307                     190310"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Restrict to immune regions\n",
    "#-------------- CHANGE HERE FOR DIFFERENT REGION SUBSET ----------------------\n",
    "# BLUEPRINT immune regions\n",
    "ATLAS_PATH = PROJECT_DIR + f'/output/deconv_inhouse_v2.atlas.tsv.gz'\n",
    "atlas = pd.read_csv(ATLAS_PATH, sep='\\t')\n",
    "subset_region_set = set(atlas.region_id)\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# filter regions down to regions of interest\n",
    "region_df = region_df[region_df['region_id'].isin(subset_region_set)]\n",
    "region_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragment Level Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(directory):\n",
    "\n",
    "    list_paths = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        list_paths.append(os.path.abspath(os.path.join(directory, filename)))\n",
    "\n",
    "    return(list_paths)\n",
    "\n",
    "\n",
    "def compute_frag_scores(cpg_number_cutoff: int) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that returns a function, used for reduce\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_frag_scores_inner(pat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        data = pat_df.copy()\n",
    "        data['offset_min'] = (data['region_cpg_index_min'] - data['cpg_index_min']).clip(lower=0)\n",
    "        data['offset_max'] = np.minimum(\n",
    "            data['region_cpg_index_max'] - data['cpg_index_min'], \n",
    "            data['cpg_index_max'] - data['cpg_index_min'])\n",
    "        data['trimmed_pat'] = data.apply(lambda x: x['pat_string'][x['offset_min']:(x['offset_max']+1)], axis=1)\n",
    "        #--- Filter molecules based on observed CpG loci\n",
    "        observed_cpg_number = (data['trimmed_pat'].str.count('C')+data['trimmed_pat'].str.count('T'))\n",
    "        ridxs = (observed_cpg_number>=cpg_number_cutoff)\n",
    "        data = data[ridxs].copy()\n",
    "        if (data.shape[0]>0):\n",
    "            # Compute k-mer methylation states\n",
    "            for k in KMERS:\n",
    "                data['meth_k%i'%k] = data['trimmed_pat']\\\n",
    "                    .apply(lambda x: len(re.findall('[C]{%i}'%k, x, overlapped=True)))\n",
    "                data['unmeth_k%i'%k] = data['trimmed_pat']\\\n",
    "                    .apply(lambda x: len(re.findall('[T]{%i}'%k, x, overlapped=True)))\n",
    "                data['total_k%i'%k] = data['trimmed_pat']\\\n",
    "                    .apply(lambda x: len(re.findall('[TC]{%i}'%k, x, overlapped=True)))\n",
    "            # Compute alpha distribution metrics\n",
    "            data['alpha'] = data['meth_k1']/data['total_k1']\n",
    "            for rate in RATES_LEQ:\n",
    "                data['frac_alpha_leq_%ipct'%(100*rate)] = np.where(data['alpha']<=rate, 1, 0)\n",
    "            for rate in RATES_GEQ:\n",
    "                data['frac_alpha_geq_%ipct'%(100*rate)] = np.where(data['alpha']>=rate, 1, 0)\n",
    "            # Expand entries that correspond to multiple molecules\n",
    "            data['number_molecules'] = data['number_molecules'].apply(lambda x: list(range(x)))\n",
    "            data = data.explode('number_molecules')\n",
    "            data['number_molecules'] = 1\n",
    "            # Aggregate metrics\n",
    "            #rv = data.groupby(['region_id', 'sample_id'])\\\n",
    "            rv = data.groupby(['region_id'])\\\n",
    "                [['meth_k1', 'unmeth_k1', 'total_k1',\n",
    "                  'meth_k3', 'unmeth_k3', 'total_k3',\n",
    "                  'meth_k4', 'unmeth_k4', 'total_k4',\n",
    "                  'frac_alpha_leq_25pct', 'frac_alpha_geq_75pct', 'number_molecules']].sum()\\\n",
    "                .reset_index()\n",
    "            rv['frac_alpha_leq_25pct'] = rv['frac_alpha_leq_25pct']/rv['number_molecules']\n",
    "            rv['frac_alpha_geq_75pct'] = rv['frac_alpha_geq_75pct']/rv['number_molecules']\n",
    "        else:\n",
    "            rv = pd.DataFrame(columns=RETURN_SCHEMA.names)\n",
    "                      \n",
    "        \n",
    "        return rv[RETURN_SCHEMA.names]\n",
    "\n",
    "    return compute_frag_scores_inner\n",
    "\n",
    "\n",
    "def score_matrix(parquet_path, result_path, pat_cols, region_df, batch_size, schema, save=False, verbose=False):\n",
    "    '''\n",
    "    one parquet file --> one score matrix\n",
    "    '''\n",
    "    # Load single parquet file\n",
    "    pat_df = spark.read.parquet(parquet_path).select(*pat_cols)\n",
    "    \n",
    "    # Compute scores by batch\n",
    "    region_df['batch'] = (np.arange(region_df.shape[0])/batch_size).astype(int)\n",
    "    rv_scores = list()\n",
    "    \n",
    "    for batch, batch_region_df in region_df.groupby('batch'):\n",
    "        rv_ov = list()\n",
    "        if verbose: print('--------------> Processing batch %i...' % batch)\n",
    "        for _, row in batch_region_df.iterrows():\n",
    "            ov_ddf = pat_df.filter(col('cpg_index_min')<=row['region_cpg_index_max_hg38'])\\\n",
    "                .filter(col('cpg_index_max') >= row['region_cpg_index_min_hg38'])\\\n",
    "                .withColumn('region_id', lit(row['region_id']))\\\n",
    "                .withColumn('region_cpg_index_min', lit(row['region_cpg_index_min_hg38']))\\\n",
    "                .withColumn('region_cpg_index_max', lit(row['region_cpg_index_max_hg38']))\n",
    "            rv_ov.append(ov_ddf)\n",
    "        scores_df = functools.reduce(DataFrame.union, rv_ov)\\\n",
    "            .groupby('region_id')\\\n",
    "            .applyInPandas(compute_frag_scores_udf, schema=RETURN_SCHEMA)\\\n",
    "            .toPandas()\n",
    "        rv_scores.append(scores_df)\n",
    "    \n",
    "    scores_df = pd.concat(rv_scores)\n",
    "    \n",
    "    if save:\n",
    "        file_name = os.path.basename(parquet_path)\n",
    "        file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "        save_path = result_path + '/' + file_name_without_ext + '.tsv.gz'\n",
    "        scores_df.to_csv(save_path, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "def score_matrix_n_times(mix_dir_path, result_path, pat_cols, region_df, batch_size, schema, save=False, verbose=False):\n",
    "    '''\n",
    "    mixture directory of replicate mixture parquets --> score matrix per replicate mixture parquet\n",
    "    '''\n",
    "    \n",
    "    # create result directory   \n",
    "    if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)\n",
    "    \n",
    "    # given directory path grab all parquet --> load path strings into a list\n",
    "    list_parquet_paths = get_file_paths(mix_dir_path)\n",
    "    \n",
    "    # for each parquet in the list run score_matrix\n",
    "    for path in list_parquet_paths:\n",
    "        file_name = os.path.basename(path)\n",
    "        file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "        print(f'--------> Computing score matrix for {file_name_without_ext}')\n",
    "            \n",
    "        score_matrix(parquet_path=path,\n",
    "                    result_path=result_path,\n",
    "                    pat_cols=pat_cols, \n",
    "                    region_df=region_df, \n",
    "                    batch_size=batch_size, \n",
    "                    schema=schema, \n",
    "                    save=save, \n",
    "                    verbose=verbose) \n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def score_matrix_from_mixture_directory(path_to_mixture_dir, result_path, pat_cols, region_df, batch_size, schema, save=False, verbose=False):\n",
    "    '''\n",
    "    dir_path_to_experiment\n",
    "    '''\n",
    "    print(f'>>> Start computing score matrices <<< \\n')\n",
    "    \n",
    "    # create result directory\n",
    "    result_dir_path = result_path + 'methyl_score/'\n",
    "    \n",
    "    if not os.path.exists(result_dir_path):\n",
    "        os.mkdir(result_dir_path)\n",
    "        \n",
    "    # given directory path grab all mixture directories containing parquet\n",
    "    list_mixture_dir_paths = get_file_paths(path_to_mixture_dir)\n",
    "    \n",
    "    # iterate through each mixture proportion directory\n",
    "    for path in list_mixture_dir_paths:\n",
    "        \n",
    "        mixture_dir_name = os.path.basename(path)\n",
    "        file_name_without_ext = os.path.splitext(mixture_dir_name)[0]\n",
    "        save_path = result_dir_path + file_name_without_ext + '/'\n",
    "        \n",
    "        print(f'--> {file_name_without_ext}')\n",
    "\n",
    "        score_matrix_n_times(mix_dir_path=path, \n",
    "                             result_path=save_path,\n",
    "                             pat_cols=pat_cols, \n",
    "                             region_df=region_df, \n",
    "                             batch_size=batch_size, \n",
    "                             schema=schema, \n",
    "                             save=save, \n",
    "                             verbose=verbose)\n",
    "        \n",
    "    print('>>> Complete. <<< \\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Start computing score matrices <<< \n",
      "\n",
      "--> E1B_E18CD4_E18CD8_E18NK_E18MONO_E18NEUTRO\n",
      "--------> Computing score matrix for mix0_seed512070\n",
      "--------> Computing score matrix for mix1_seed150400\n",
      "--------> Computing score matrix for mix2_seed53691\n",
      "\n",
      "\n",
      "--> E01B_E198CD4_E198CD8_E198NK_E198MONO_E198NEUTRO\n",
      "--------> Computing score matrix for mix0_seed776570\n",
      "--------> Computing score matrix for mix1_seed581495\n",
      "--------> Computing score matrix for mix2_seed787335\n",
      "\n",
      "\n",
      "--> E001B_E1998CD4_E1998CD8_E1998NK_E1998MONO_E1998NEUTRO\n",
      "--------> Computing score matrix for mix0_seed372142\n",
      "--------> Computing score matrix for mix1_seed285922\n",
      "--------> Computing score matrix for mix2_seed689569\n",
      "\n",
      "\n",
      ">>> Complete. <<< \n",
      "\n",
      "CPU times: user 5.58 s, sys: 1.26 s, total: 6.84 s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "PAT_COLS = [\n",
    "    'molecule_id', 'chr', 'number_molecules',\n",
    "    'cpg_index_min', 'cpg_index_max', 'pat_string'\n",
    "]\n",
    "\n",
    "QUANTILES = [0.1, 0.25, 0.75, 0.9]\n",
    "KMERS = [1, 3, 4]\n",
    "RATES_LEQ = [0.25]\n",
    "RATES_GEQ = [0.75]\n",
    "\n",
    "RETURN_SCHEMA = StructType()\\\n",
    "    .add('region_id', 'string')\\\n",
    "    .add('number_molecules', 'integer')\\\n",
    "    .add('meth_k1', 'integer')\\\n",
    "    .add('unmeth_k1', 'integer')\\\n",
    "    .add('total_k1', 'integer')\\\n",
    "    .add('meth_k3', 'integer')\\\n",
    "    .add('unmeth_k3', 'integer')\\\n",
    "    .add('total_k3', 'integer')\\\n",
    "    .add('meth_k4', 'integer')\\\n",
    "    .add('unmeth_k4', 'integer')\\\n",
    "    .add('total_k4', 'integer')\\\n",
    "    .add('frac_alpha_leq_25pct', 'float')\\\n",
    "    .add('frac_alpha_geq_75pct', 'float')\n",
    "\n",
    "compute_frag_scores_udf = compute_frag_scores(cpg_number_cutoff=FILTER_CG_COUNT)\n",
    "\n",
    "#--- Local paths\n",
    "ROOT_DIR = '/analysis/gh-msun/projects'\n",
    "PROJECT_SLUG = '2023_06_26_SRT_deconvolution_MS/'\n",
    "PROJECT_DIR = ROOT_DIR + '/{}'.format(PROJECT_SLUG)\n",
    "EXPERIMENT_NAME = 'BLUEPRINT_B'\n",
    "PATH_TO_MIXTURE_DIR = PROJECT_DIR + f'output/experiment/{EXPERIMENT_NAME}/mixture/'\n",
    "RESULT_PATH = PROJECT_DIR + f'output/experiment/{EXPERIMENT_NAME}/'\n",
    "\n",
    "# compute methyl score for are parquet files\n",
    "score_matrix_from_mixture_directory(path_to_mixture_dir = PATH_TO_MIXTURE_DIR, \n",
    "                                   result_path = RESULT_PATH, \n",
    "                                   pat_cols = PAT_COLS, \n",
    "                                   region_df = region_df, \n",
    "                                   batch_size = 20, \n",
    "                                   schema = RETURN_SCHEMA, \n",
    "                                   save=True, \n",
    "                                   verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
