{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Frag Meth Summaries for a Region Set\n",
    "\n",
    "Given a region set will compute frag scores for all samples in PAT Parquet files. Here we use hg38 PARQUET files.\n",
    "\n",
    "SET UP SDDs TO BE USED WITH SPARK PRIOR TO RUNNING THIS NOTEBOOK!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "import regex as re\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = 'deconvolution_v2.v23_conv.with_cpg_index'\n",
    "REGION_BED_COLS = [\n",
    "    'region_chr', 'region_start', 'region_end', \n",
    "    'region_cpg_index_min', 'region_cpg_index_max', 'region_id'\n",
    "]\n",
    "FILTER_CG_COUNT = 3\n",
    "FILTER_CG_COUNT_REGION = 1\n",
    "\n",
    "#--- Local paths\n",
    "ROOT_DIR = '/analysis/gh-msun/projects'\n",
    "PROJECT_SLUG = '2023_06_26_SRT_deconvolution_MS'\n",
    "PROJECT_DIR = ROOT_DIR + '/{}'.format(PROJECT_SLUG)\n",
    "\n",
    "# Regions\n",
    "REGION_PATH = (\n",
    "    PROJECT_DIR + '/stage/panel_data/{regions}.bed'\n",
    ").format(regions=REGIONS)\n",
    "\n",
    "# CpG map; genomic coordinate to CpG index;\n",
    "CPG_MAP_PATH = PROJECT_DIR + '/stage/cpg_loci/cpg_loci_hg19.combined_annot.tsv.gz'\n",
    "\n",
    "\n",
    "# BLUEPRINT HG38: s3://gh-bi-lunar/public_data/blueprint/hg38_20160816.pat.db_version.parquet/\n",
    "PARQUET_PATH_LIST_HG38 = [\n",
    "    '/analysis/hg38_20160816.pat.db_version.parquet'\n",
    "]\n",
    "\n",
    "#--- Where to store results\n",
    "RESULTS_PATH = (\n",
    "    PROJECT_DIR + '/output/meth_summaries/blueprint_meth_summaries_cg_count_geq_{k}_{regions}.tsv.gz'\n",
    ").format(regions=REGIONS, k=FILTER_CG_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/analysis/gh-msun/projects/2023_06_26_SRT_deconvolution_MS/output/meth_summaries/blueprint_meth_summaries_cg_count_geq_3_deconvolution_v2.v23_conv.with_cpg_index.tsv.gz'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import IntegerType, LongType, ArrayType, StringType, DoubleType\n",
    "from pyspark.sql.functions import udf, explode, broadcast, count, lit, length, col\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "## this works for PySpark v3.3.1 - only need to run this once\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages {aws_java},{aws_hadoop} pyspark-shell\".\\\n",
    "   format(aws_java=\"com.amazonaws:aws-java-sdk-bundle:1.11.271\",\n",
    "          aws_hadoop=\"org.apache.hadoop:hadoop-aws:3.1.2\")\n",
    "#####\n",
    "\n",
    "# UPDATE HOME!\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark\"\n",
    "# THIS needs to be set-up before running the notebook\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"/temp\"\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.executor.instances\", \"2\")\n",
    "spark_conf.set(\"spark.executor.cores\", \"2\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"64g\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "spark_conf.set(\"spark.parquet.filterPushdown\", \"true\")\n",
    "spark_conf.set(\"spark.local.dir\", \"/temp\")\n",
    "spark_conf.getAll()\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CpG Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpg_map = pd.read_csv(CPG_MAP_PATH, usecols=['chr', 'start', 'end', 'cpg_index', 'cpg_index_hg38'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 5.57 s, total: 20.3 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridxs = ~cpg_map['cpg_index_hg38'].isna()\n",
    "hg19_hg38_map = dict(itertools.zip_longest(cpg_map[ridxs]['cpg_index'], cpg_map[ridxs]['cpg_index_hg38'].astype(int)))\n",
    "hg38_hg19_map = dict(itertools.zip_longest(cpg_map[ridxs]['cpg_index_hg38'].astype(int), cpg_map[ridxs]['cpg_index']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 1658)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df = pd.read_csv(REGION_PATH, sep='\\t', usecols=range(0, 6), names=REGION_BED_COLS)\n",
    "\n",
    "region_df['region_cpg_index_max'] -= 1\n",
    "region_df.sort_values('region_cpg_index_min', inplace=True)\n",
    "region_df['region_cpg_index_min_hg38'] = region_df['region_cpg_index_min'].map(hg19_hg38_map)\n",
    "region_df['region_cpg_index_max_hg38'] = region_df['region_cpg_index_max'].map(hg19_hg38_map)\n",
    "\n",
    "region_df.shape[0], region_df['region_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 1658)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridxs = ~region_df['region_cpg_index_min_hg38'].isna()\n",
    "ridxs &= ~region_df['region_cpg_index_max_hg38'].isna()\n",
    "region_df = region_df[ridxs].copy()\n",
    "region_df.shape[0], region_df['region_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1657, 1657)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg_count_hg19 = region_df['region_cpg_index_max']-region_df['region_cpg_index_min'] + 1\n",
    "cg_count_hg38 = region_df['region_cpg_index_max_hg38']-region_df['region_cpg_index_min_hg38'] + 1\n",
    "ridxs = (cg_count_hg19==cg_count_hg38)\n",
    "ridxs &= (cg_count_hg19>=FILTER_CG_COUNT_REGION)\n",
    "region_df = region_df[ridxs].copy()\n",
    "region_df.shape[0], region_df['region_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df['region_cpg_index_min_hg38'] = region_df['region_cpg_index_min_hg38'].astype(int)\n",
    "region_df['region_cpg_index_max_hg38'] = region_df['region_cpg_index_max_hg38'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAT PARQUET Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_COLS = [\n",
    "    'sample_id', 'molecule_id', 'chr', 'number_molecules',\n",
    "    'cpg_index_min', 'cpg_index_max', 'pat_string'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sample_id: string (nullable = true)\n",
      " |-- molecule_id: string (nullable = true)\n",
      " |-- chr: string (nullable = true)\n",
      " |-- number_molecules: integer (nullable = true)\n",
      " |-- cpg_index_min: long (nullable = true)\n",
      " |-- cpg_index_max: long (nullable = true)\n",
      " |-- pat_string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pat_parquet_files = [spark.read.parquet(ifile).select(*PAT_COLS) for ifile in PARQUET_PATH_LIST_HG38]\n",
    "pat_hg38_ddf = functools.reduce(DataFrame.unionByName, pat_parquet_files)\n",
    "pat_hg38_ddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----+----------------+-------------+-------------+--------------------+\n",
      "|sample_id|molecule_id| chr|number_molecules|cpg_index_min|cpg_index_max|          pat_string|\n",
      "+---------+-----------+----+----------------+-------------+-------------+--------------------+\n",
      "|ERS337091|          2|chr1|               1|           17|           57|TTCTCCTTTCCCCCTTC...|\n",
      "|ERS337091|          1|chr1|               1|           19|           58|TTCCCCCTTCTCCTTTC...|\n",
      "|ERS337091|          3|chr1|               1|           99|          115|   CCCCCCCCCCCCCCCTT|\n",
      "|ERS337091|          6|chr1|               1|          101|          120|CCTCCCCCCCC....TTCTT|\n",
      "|ERS337091|          5|chr1|               1|          108|          118|         CCCCCCTCCTC|\n",
      "+---------+-----------+----+----------------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pat_hg38_ddf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragment Level Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTILES = [0.1, 0.25, 0.75, 0.9]\n",
    "KMERS = [1, 3, 4]\n",
    "RATES_LEQ = [0.25]\n",
    "RATES_GEQ = [0.75]\n",
    "\n",
    "RETURN_SCHEMA = StructType()\\\n",
    "    .add('sample_id', 'string')\\\n",
    "    .add('region_id', 'string')\\\n",
    "    .add('number_molecules', 'integer')\\\n",
    "    .add('meth_k1', 'integer')\\\n",
    "    .add('unmeth_k1', 'integer')\\\n",
    "    .add('total_k1', 'integer')\\\n",
    "    .add('meth_k3', 'integer')\\\n",
    "    .add('unmeth_k3', 'integer')\\\n",
    "    .add('total_k3', 'integer')\\\n",
    "    .add('meth_k4', 'integer')\\\n",
    "    .add('unmeth_k4', 'integer')\\\n",
    "    .add('total_k4', 'integer')\\\n",
    "    .add('frac_alpha_leq_25pct', 'float')\\\n",
    "    .add('frac_alpha_geq_75pct', 'float')\n",
    "\n",
    "def compute_frag_scores(cpg_number_cutoff: int) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that returns a function, used for reduce\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_frag_scores_inner(pat_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        data = pat_df.copy()\n",
    "        data['offset_min'] = (data['region_cpg_index_min'] - data['cpg_index_min']).clip(lower=0)\n",
    "        data['offset_max'] = np.minimum(\n",
    "            data['region_cpg_index_max'] - data['cpg_index_min'], \n",
    "            data['cpg_index_max'] - data['cpg_index_min'])\n",
    "        data['trimmed_pat'] = data.apply(lambda x: x['pat_string'][x['offset_min']:(x['offset_max']+1)], axis=1)\n",
    "        #--- Filter molecules based on observed CpG loci\n",
    "        observed_cpg_number = (data['trimmed_pat'].str.count('C')+data['trimmed_pat'].str.count('T'))\n",
    "        ridxs = (observed_cpg_number>=cpg_number_cutoff)\n",
    "        data = data[ridxs].copy()\n",
    "        if (data.shape[0]>0):\n",
    "            # Compute k-mer methylation states\n",
    "            for k in KMERS:\n",
    "                data['meth_k%i'%k] = data['trimmed_pat']\\\n",
    "                    .apply(lambda x: len(re.findall('[C]{%i}'%k, x, overlapped=True)))\n",
    "                data['unmeth_k%i'%k] = data['trimmed_pat']\\\n",
    "                    .apply(lambda x: len(re.findall('[T]{%i}'%k, x, overlapped=True)))\n",
    "                data['total_k%i'%k] = data['trimmed_pat']\\\n",
    "                    .apply(lambda x: len(re.findall('[TC]{%i}'%k, x, overlapped=True)))\n",
    "            # Compute alpha distribution metrics\n",
    "            data['alpha'] = data['meth_k1']/data['total_k1']\n",
    "            for rate in RATES_LEQ:\n",
    "                data['frac_alpha_leq_%ipct'%(100*rate)] = np.where(data['alpha']<=rate, 1, 0)\n",
    "            for rate in RATES_GEQ:\n",
    "                data['frac_alpha_geq_%ipct'%(100*rate)] = np.where(data['alpha']>=rate, 1, 0)\n",
    "            # Expand entries that correspond to multiple molecules\n",
    "            data['number_molecules'] = data['number_molecules'].apply(lambda x: list(range(x)))\n",
    "            data = data.explode('number_molecules')\n",
    "            data['number_molecules'] = 1\n",
    "            # Aggregate metrics\n",
    "            rv = data.groupby(['region_id', 'sample_id'])\\\n",
    "                [['meth_k1', 'unmeth_k1', 'total_k1',\n",
    "                  'meth_k3', 'unmeth_k3', 'total_k3',\n",
    "                  'meth_k4', 'unmeth_k4', 'total_k4',\n",
    "                  'frac_alpha_leq_25pct', 'frac_alpha_geq_75pct', 'number_molecules']].sum()\\\n",
    "                .reset_index()\n",
    "            rv['frac_alpha_leq_25pct'] = rv['frac_alpha_leq_25pct']/rv['number_molecules']\n",
    "            rv['frac_alpha_geq_75pct'] = rv['frac_alpha_geq_75pct']/rv['number_molecules']\n",
    "        else:\n",
    "            rv = pd.DataFrame(columns=RETURN_SCHEMA.names)\n",
    "                      \n",
    "        \n",
    "        return rv[RETURN_SCHEMA.names]\n",
    "\n",
    "    return compute_frag_scores_inner\n",
    "\n",
    "\n",
    "compute_frag_scores_udf = compute_frag_scores(cpg_number_cutoff=FILTER_CG_COUNT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute for HG38 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Processing batch 0...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 20\n",
    "region_df['batch'] = (np.arange(region_df.shape[0])/BATCH_SIZE).astype(int)\n",
    "rv_scores = list()\n",
    "for batch, batch_region_df in region_df.groupby('batch'):\n",
    "    rv_ov = list()\n",
    "    print('---> Processing batch %i...' % batch)\n",
    "    for _, row in batch_region_df.iterrows():\n",
    "        ov_ddf = pat_hg38_ddf.filter(col('cpg_index_min')<=row['region_cpg_index_max_hg38'])\\\n",
    "            .filter(col('cpg_index_max') >= row['region_cpg_index_min_hg38'])\\\n",
    "            .withColumn('region_id', lit(row['region_id']))\\\n",
    "            .withColumn('region_cpg_index_min', lit(row['region_cpg_index_min_hg38']))\\\n",
    "            .withColumn('region_cpg_index_max', lit(row['region_cpg_index_max_hg38']))\n",
    "        rv_ov.append(ov_ddf)\n",
    "    scores_df = functools.reduce(DataFrame.union, rv_ov)\\\n",
    "        .groupby('region_id')\\\n",
    "        .applyInPandas(compute_frag_scores_udf, schema=RETURN_SCHEMA)\\\n",
    "        .toPandas()\n",
    "    rv_scores.append(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_hg38_df = pd.concat(rv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206303, 1648, 127)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = scores_hg38_df\n",
    "scores_df.shape[0], scores_df['region_id'].nunique(), scores_df['sample_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.16 s, sys: 0 ns, total: 3.16 s\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores_df.to_csv(RESULTS_PATH,\n",
    "                 sep='\\t', \n",
    "                 index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206303, 14)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "309.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
