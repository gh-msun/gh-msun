{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for taking raw parquet file reads and subsetting to regions of interest \\\n",
    "and collapsing samples by cell type. This notebook outputs parquet files by celltype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import itertools\n",
    "import functools\n",
    "import os\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import IntegerType, LongType, ArrayType, StringType, DoubleType\n",
    "from pyspark.sql.functions import udf, explode, broadcast, count, lit, length, col, expr\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Local paths\n",
    "ROOT_DIR = '/analysis/gh-msun/projects'\n",
    "PROJECT_SLUG = '2023_06_26_SRT_deconvolution_MS'\n",
    "PROJECT_DIR = ROOT_DIR + f'/{PROJECT_SLUG}'\n",
    "DATA_DIR = ROOT_DIR + f'/{PROJECT_SLUG}' + '/stage'\n",
    "SAMPLE_PATH = DATA_DIR + '/metadata/samples_wgbs.20230329.tsv'\n",
    "\n",
    "#--- parquet\n",
    "PARQUET_PATH_LIST_HG38 = [\n",
    "    '/analysis/data/hg38_20160816.pat.db_version.parquet'\n",
    "]\n",
    "\n",
    "#--- regions\n",
    "REGIONS = 'deconvolution_v2.v23_conv.with_cpg_index'\n",
    "\n",
    "REGION_BED_COLS = [\n",
    "    'region_chr', 'region_start', 'region_end', \n",
    "    'region_cpg_index_min', 'region_cpg_index_max', 'region_id'\n",
    "]\n",
    "\n",
    "REGION_PATH = (\n",
    "    PROJECT_DIR + '/stage/panel_data/{regions}.bed'\n",
    ").format(regions=REGIONS)\n",
    "\n",
    "# CpG map; genomic coordinate to CpG index;\n",
    "CPG_MAP_PATH = PROJECT_DIR + '/stage/cpg_loci/cpg_loci_hg19.combined_annot.tsv.gz'\n",
    "\n",
    "#-- output\n",
    "RESULTS_PATH='/analysis/gh-msun/projects/2023_06_26_SRT_deconvolution_MS/output/mixture_source/t_subset_mixture_source/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset parquets to regions of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load regions we want to subset\n",
    "Load regions of interest to subset. e.g. The ATLAS dataframe contains BLUEPRINT immune regions only. The regions we want to subset to should be represented as a set of region id called `subset_region_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #-------------- CHANGE HERE FOR DIFFERENT REGION SUBSET ----------------------\n",
    "# # BLUEPRINT immune regions\n",
    "# ATLAS_PATH = PROJECT_DIR + f'/output/deconv_inhouse_v2.atlas.tsv.gz'\n",
    "# atlas = pd.read_csv(ATLAS_PATH, sep='\\t')\n",
    "# subset_region_set = set(atlas.region_id)\n",
    "# #-----------------------------------------------------------------------------\n",
    "\n",
    "# # filter regions down to regions of interest\n",
    "# region_df = pd.read_csv(REGION_PATH, sep='\\t', usecols=range(0, 6), names=REGION_BED_COLS)\n",
    "# region_df_subset = region_df[region_df['region_id'].isin(subset_region_set)]\n",
    "# region_df_subset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.9 s, sys: 6.18 s, total: 46.1 s\n",
      "Wall time: 46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cpg_map = pd.read_csv(CPG_MAP_PATH, usecols=['chr', 'start', 'end', 'cpg_index', 'cpg_index_hg38'], sep='\\t')\n",
    "ridxs = ~cpg_map['cpg_index_hg38'].isna()\n",
    "hg19_hg38_map = dict(itertools.zip_longest(cpg_map[ridxs]['cpg_index'], cpg_map[ridxs]['cpg_index_hg38'].astype(int)))\n",
    "hg38_hg19_map = dict(itertools.zip_longest(cpg_map[ridxs]['cpg_index_hg38'].astype(int), cpg_map[ridxs]['cpg_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df = pd.read_csv(REGION_PATH, sep='\\t', usecols=range(0, 6), names=REGION_BED_COLS)\n",
    "region_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_chr</th>\n",
       "      <th>region_start</th>\n",
       "      <th>region_end</th>\n",
       "      <th>region_cpg_index_min</th>\n",
       "      <th>region_cpg_index_max</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1114771</td>\n",
       "      <td>1114971</td>\n",
       "      <td>20117</td>\n",
       "      <td>20130</td>\n",
       "      <td>Immune_Broad_B-chr1:1114772-1114971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1157450</td>\n",
       "      <td>1157720</td>\n",
       "      <td>21684</td>\n",
       "      <td>21704</td>\n",
       "      <td>Immune_Broad_NK-chr1:1157451-1157720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1157879</td>\n",
       "      <td>1158277</td>\n",
       "      <td>21710</td>\n",
       "      <td>21727</td>\n",
       "      <td>Immune_Broad_NK-chr1:1157880-1158277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1652503</td>\n",
       "      <td>1652793</td>\n",
       "      <td>41590</td>\n",
       "      <td>41599</td>\n",
       "      <td>Loyfer2022_Preprint_Colon-Ep:Gastric-Ep:Small-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>1849567</td>\n",
       "      <td>1849674</td>\n",
       "      <td>46692</td>\n",
       "      <td>46698</td>\n",
       "      <td>Pancreas_Acinar-chr1:1849568-1849674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region_chr  region_start  region_end  region_cpg_index_min  region_cpg_index_max                                          region_id\n",
       "0       chr1       1114771     1114971                 20117                 20130                Immune_Broad_B-chr1:1114772-1114971\n",
       "1       chr1       1157450     1157720                 21684                 21704               Immune_Broad_NK-chr1:1157451-1157720\n",
       "2       chr1       1157879     1158277                 21710                 21727               Immune_Broad_NK-chr1:1157880-1158277\n",
       "3       chr1       1652503     1652793                 41590                 41599  Loyfer2022_Preprint_Colon-Ep:Gastric-Ep:Small-...\n",
       "4       chr1       1849567     1849674                 46692                 46698               Pancreas_Acinar-chr1:1849568-1849674"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.5 s, sys: 3.4 s, total: 34.9 s\n",
      "Wall time: 34.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1658, 1658)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "region_df['region_cpg_index_max'] -= 1\n",
    "region_df.sort_values('region_cpg_index_min', inplace=True)\n",
    "region_df['region_cpg_index_min_hg38'] = region_df['region_cpg_index_min'].map(hg19_hg38_map)\n",
    "region_df['region_cpg_index_max_hg38'] = region_df['region_cpg_index_max'].map(hg19_hg38_map)\n",
    "\n",
    "region_df.shape[0], region_df['region_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parquet file as a pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# UPDATE HOME!\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/ec2-user/mambaforge/envs/2023_06_26_SRT_deconvolution_MS/lib/python3.7/site-packages/pyspark\"\n",
    "# THIS needs to be set-up before running the notebook\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"/temp\"\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.ui.showConsoleProgress\", \"True\")\n",
    "spark_conf.set(\"spark.executor.instances\", \"2\")\n",
    "spark_conf.set(\"spark.executor.cores\", \"2\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"64g\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"32g\")\n",
    "spark_conf.set(\"spark.parquet.filterPushdown\", \"true\")\n",
    "spark_conf.set(\"spark.local.dir\", \"/temp\")\n",
    "spark_conf.getAll()\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_COLS = [\n",
    "    'sample_id', 'molecule_id', 'chr', 'number_molecules',\n",
    "    'cpg_index_min', 'cpg_index_max', 'pat_string'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sample_id: string (nullable = true)\n",
      " |-- molecule_id: string (nullable = true)\n",
      " |-- chr: string (nullable = true)\n",
      " |-- number_molecules: integer (nullable = true)\n",
      " |-- cpg_index_min: long (nullable = true)\n",
      " |-- cpg_index_max: long (nullable = true)\n",
      " |-- pat_string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pat_parquet_files = [spark.read.parquet(ifile).select(*PAT_COLS) for ifile in PARQUET_PATH_LIST_HG38]\n",
    "pat_hg38_ddf = functools.reduce(DataFrame.unionByName, pat_parquet_files)\n",
    "pat_hg38_ddf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for duplicate molecules\n",
    "# pat_hg38_ddf.count()\n",
    "# pat_hg38_ddf.filter(col(\"number_molecules\") == 1).count()\n",
    "# pat_hg38_ddf.filter(col(\"number_molecules\") > 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe of reads that fall in the subsetted regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Processing batch 0...\n",
      "---> Processing batch 1...\n",
      "---> Processing batch 2...\n",
      "---> Processing batch 3...\n",
      "---> Processing batch 4...\n",
      "---> Processing batch 5...\n",
      "---> Processing batch 6...\n",
      "---> Processing batch 7...\n",
      "---> Processing batch 8...\n",
      "---> Processing batch 9...\n",
      "---> Processing batch 10...\n",
      "---> Processing batch 11...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 20\n",
    "region_df['batch'] = (np.arange(region_df.shape[0])/BATCH_SIZE).astype(int)\n",
    "rv_scores = list()\n",
    "for batch, batch_region_df in region_df.groupby('batch'):\n",
    "    rv_ov = list()\n",
    "    print('---> Processing batch %i...' % batch)\n",
    "    for _, row in batch_region_df.iterrows():\n",
    "        ov_ddf = pat_hg38_ddf.filter(col('cpg_index_min')<=row['region_cpg_index_max_hg38'])\\\n",
    "            .filter(col('cpg_index_max') >= row['region_cpg_index_min_hg38'])\\\n",
    "            .withColumn('region_id', lit(row['region_id']))\\\n",
    "            .withColumn('region_cpg_index_min', lit(row['region_cpg_index_min_hg38']))\\\n",
    "            .withColumn('region_cpg_index_max', lit(row['region_cpg_index_max_hg38']))\n",
    "        rv_ov.append(ov_ddf)\n",
    "        \n",
    "    scores_df = functools.reduce(DataFrame.union, rv_ov)\n",
    "    scores_df_pandas = scores_df.toPandas()\n",
    "    scores_df_spark = spark.createDataFrame(scores_df_pandas) \n",
    "    \n",
    "    rv_scores.append(scores_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "combined_df = functools.reduce(DataFrame.union, rv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_scores[0].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_scores[2].filter(col(\"number_molecules\") == 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create parquet file for each cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSource = 'blueprint_loyfer2022'\n",
    "\n",
    "t_reg_list = ['regulatory T cell']\n",
    "\n",
    "t_naive_list = ['CD4-positive, alpha-beta T cell',\n",
    "                'CD8-positive, alpha-beta T cell' ]\n",
    "\n",
    "t_effector_list = ['effector memory CD8-positive, alpha-beta T cell', \n",
    "                 'effector memory CD8-positive, alpha-beta T cell, terminally differentiated',\n",
    "                 'central memory CD8-positive, alpha-beta T cell']\n",
    "\n",
    "cellType = [   \n",
    "    'Blueprint-B',\n",
    "    't_reg',\n",
    "    't_naive',\n",
    "    't_effector',\n",
    "    'Blueprint-NK',\n",
    "    'Blueprint-Dend',\n",
    "    'Blueprint-Macro',\n",
    "    'Blueprint-Mono',\n",
    "    'Blueprint-Eosi',\n",
    "    'Blueprint-Neutro',\n",
    "    'Blueprint-Eryth',\n",
    "    'Blueprint-Mega',\n",
    "  #  'Eryth-prog'\n",
    "]\n",
    "\n",
    "# get samples that fall under these cell types of interest\n",
    "sample_df = pd.read_csv(SAMPLE_PATH, sep='\\t')\n",
    "\n",
    "\n",
    "for index, row in sample_df.iterrows():\n",
    "    \n",
    "    if row['cell_type'] in t_reg_list:\n",
    "        sample_df.at[index, 'sample_group'] = 't_reg'\n",
    "        \n",
    "    if row['cell_type'] in t_naive_list:\n",
    "        sample_df.at[index, 'sample_group'] = 't_naive'\n",
    "        \n",
    "    if row['cell_type'] in t_effector_list:\n",
    "        sample_df.at[index, 'sample_group'] = 't_effector'\n",
    "\n",
    "ridxs = (sample_df['source']==dataSource)\n",
    "ridxs &= sample_df['sample_group'].isin(cellType)\n",
    "ref_sample_df = sample_df[ridxs].copy()\n",
    "ref_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples do each cell type have?\n",
    "ref_sample_df.groupby('sample_group').sample_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# ---- loop for collapsing samples by cell type\n",
    "region_read_count = []\n",
    "\n",
    "for celltype in cellType:\n",
    "    \n",
    "    print(f'--> Processing: ### {celltype} ###')\n",
    "\n",
    "    # get samples in the specified cell type\n",
    "    samples_by_celltype = list(ref_sample_df[ref_sample_df['sample_group'] == celltype]['sample_id'])\n",
    "    sample_size = len(samples_by_celltype)\n",
    "    print(f'-------> sample size: {sample_size}')\n",
    "\n",
    "    # filter reads that are from samples in samples_by_celltype\n",
    "    print(f'-------> filtering reads')\n",
    "    df_celltype = combined_df.filter((col('sample_id')).isin(samples_by_celltype))\n",
    "    \n",
    "#     # explode rows by number_molecules (for random sampling in mixing)\n",
    "#     print(f'-------> exploding reads')\n",
    "#     df_celltype = df_celltype.withColumn('number_molecules', expr('explode(array_repeat(number_molecules,int(number_molecules)))'))\n",
    "    \n",
    "    # save parquet file\n",
    "    print(f'-------> saving as parquet files')\n",
    "    RESULT_PARQUET_PATH = RESULTS_PATH + 'collapsed_reads_' + celltype + '/'\n",
    "    df_celltype.write.mode('overwrite').parquet(RESULT_PARQUET_PATH)\n",
    "    \n",
    "    # count reads per region\n",
    "    print(f'-------> counting reads per region')\n",
    "    read_count_by_region = df_celltype.groupBy('region_id').count().toPandas()\n",
    "    \n",
    "    # append read count \n",
    "    region_read_count.append(read_count_by_region)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "\n",
    "\n",
    "celltype_subset = ['t_reg','t_naive', 't_effector','Blueprint-B', 'Blueprint-CD4', 'Blueprint-CD8', 'Blueprint-NK', 'Blueprint-Mono', 'Blueprint-Neutro']\n",
    "\n",
    "\n",
    "matched_elements = [x for x in cellType if x in celltype_subset]\n",
    "matched_indexes = [i for i, x in enumerate(cellType) if x in celltype_subset]\n",
    "\n",
    "region_read_count_subset = [region_read_count[i] for i in matched_indexes]\n",
    "\n",
    "\n",
    "# Create a large figure\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Loop over the dataframes\n",
    "for i, (df, title) in enumerate(zip(region_read_count_subset, matched_elements), start=1):\n",
    "    # Create a subplot for each dataframe\n",
    "    ax = fig.add_subplot(4, 3, i)  \n",
    "\n",
    "    # Plot the histogram of the 'count' column using seaborn\n",
    "    sns.histplot(df['count'], bins=20, edgecolor='black', ax=ax)\n",
    "\n",
    "    # Calculate the median\n",
    "    median = df['count'].median()\n",
    "\n",
    "    # Add a vertical line for the median\n",
    "    ax.axvline(median, color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    # Add text in the upper right corner\n",
    "    ax.text(0.97, 0.97, f'Median: {median}', ha='right', va='top', transform=ax.transAxes, color='red') \n",
    "\n",
    "    # Set the title from the titles list\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel('Read count')\n",
    "\n",
    "# Add spacing between plots\n",
    "plt.tight_layout()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
